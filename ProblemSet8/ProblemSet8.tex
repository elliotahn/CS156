\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx,enumitem}
\usepackage{tikz,fancyhdr}
\usepackage[labelfont=bf]{caption}

\pagestyle{fancy}
\fancyhf{}
\chead{PROBLEM SET 8}
\rhead{Elliot Ahn}
\lhead{Machine Learning}
\rfoot{\thepage}

\setlength{\headheight}{15pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}

\begin{enumerate}[leftmargin=*]
\item (d) We minimize
\[ \frac{1}{2} \mathbf w^T \mathbf w \]
subject to the constraint
\[ y_n \left( \mathbf w^T \mathbf x_n + b \right) \geq 1 \]
The Lagrangian is
\[ \frac{1}{2} \mathbf w^T \mathbf w - \sum_n \alpha_n \left(y_n \left( \mathbf w^T \mathbf x_n + b \right) - 1 \right) \]
and the primal problem requires differentiating with respect to the $\alpha_n$'s, which gives the constraint
\[ y_n (\mathbf w^T \mathbf x_n + b) = 1 \]
If we re-absorb the $b$ into the vector $\mathbf w$ (making it a $d+1$ dimensional vector), we have the mapping
\[ \frac{1}{2} \mathbf w^T \mathbf w \to \frac{1}{2} \mathbf w^T \mathbf P \mathbf w, \qquad y_n ( \mathbf w^T \mathbf x_n + b) \to y_n \mathbf w^T \mathbf x_n \]
where $\mathbf P$ is the $(d + 1) \times (d + 1)$ matrix
\[ \mathbf P = \begin{pmatrix} 0 & \mathbf 0 \\ \mathbf 0 & \mathbf I_{d \times d} \end{pmatrix}, \qquad \text{$\mathbf I_{d \times d}$ is the identity matrix}. \]
Thus we have the $d + 1$ variable quadratic programming problem minimizing with respect to $\mathbf w$
\[ \frac{1}{2} \mathbf w^T \mathbf P \mathbf w \]
subject to the constraint
\[ y_n \mathbf w^T \mathbf x_n = 1. \]
\item (a) 0 vs all had the lowest in-sample accuracy. Interestingly, it was hard to distinguish the others from the pack. Yet, they had a lower $E_{\text{in}}$.
\item (a) 1 vs all had the lowest $E_{\text{in}}$. It was hard to distinguish the rest from the pack.
\item (c) 0 vs all has 2180 SV and 1 vs all has 386 SV, so the difference is about 1800.
\item (d) The $C = 1$ case has 2 more correct classification points in-sample than the other $C$'s.
\item (b) At $Q = 2$ the number of support vectors is $76$ while at $Q = 5$ the number of support vectors is $25$.
\item (b) $C = 0.001$ has the most lowest-$E_{\text{CV}}$ cases at 28 cases out of 100 iterations.
\item (c) Averaging $E_{\text{CV}}$ for $C = 0.001$ gives $E_{\text{CV}} = 0.00478$.
\item (e) $C = 10^6$ had the lowest $E_{\text{in}} = 0.00064$.
\item (c) $C = 100$ had the lowest $E_{\text{out}} = 0.019$.
\end{enumerate}

\end{document}